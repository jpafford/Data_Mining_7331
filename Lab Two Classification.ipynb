{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Lab Two Classification</center>\n",
    "<center><font size = \"4\"> 2017-2018 California Department of Education Mathmematics Achievement</font></center>\n",
    "\n",
    "##### <center>Create by An Nguyen, Andy Ho, Jodi Pafford</center>\n",
    "<center> March 8, 2019</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation 1\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "##Load original dataset.  No longer needed.\n",
    "##The cleaning process is very computationally expensive therefore a the 'ranifall.csv' file was created for later use. \n",
    "#rainfall_original = pd.read_csv('weatherAus.csv') \n",
    "\n",
    "#Functions to find the average value using the bracketing values around the NaN's.  \n",
    "    #For instance if a city's 'MinTemp' has 34, 32, NaN, NaN, 55 recorded \n",
    "    #the function will average 32 and 55 for the first NaN: (32+55)/2 = 43.5 \n",
    "    #and average the above value and 55 for the second NaN: (43.5+55)/2 = 49.25\n",
    "#Will only use values if they are from the same city.\n",
    "#If NaN is the earliest timepoint for a given city the next timepoint with no NaN will be given instead of the mean.\n",
    "#If NaN is the latest timepoint for a given city the previous timepoint with no NaN will be given instead of the mean.\n",
    "\n",
    "def impute_by_city(cities,variables):\n",
    "    for c in cities:\n",
    "        #aPrse out observations from a single city.\n",
    "        temp = rainfall[rainfall.Location == c]\n",
    "        \n",
    "        #Interate through all observations of the temp data file.\n",
    "        i = min(temp.index)\n",
    "        while i <= max(temp.index):\n",
    "            for v in variables:\n",
    "                #Check to see if there are values recorded for the variable, will pass over if all are NaN.\n",
    "                if pd.isna(temp[v]).all():\n",
    "                    pass\n",
    "                \n",
    "                #Check to see if a single value is NaN.\n",
    "                elif pd.isna(temp[v][i]):\n",
    "                    #Find the mean of bracketing values and impute into main dataframe.\n",
    "                    temp[v][i] = find_mean(temp[v], i)\n",
    "                    rainfall[v][i] = temp[v][i]\n",
    "            i = i + 1       \n",
    "\n",
    "#Find mean of bracketing values.\n",
    "def find_mean(templist, index):\n",
    "    #If NaN is earliest timepoint for the city take the next value that is not NaN.\n",
    "    if index == min(templist.index): \n",
    "        return find_top(templist, index)\n",
    "    \n",
    "    #If latest timepoint for the city take the previous value that is not NaN.\n",
    "    elif index == max(templist.index): \n",
    "        return find_bottom(templist, index)\n",
    "    \n",
    "    else:\n",
    "        #Find previous non-NaN value.\n",
    "        bottom = find_bottom(templist, index) \n",
    "        #Find next non-NaN value.\n",
    "        top = find_top(templist, index) \n",
    "        \n",
    "    #If current value is not from the latest timepoint for the city but there are no more non-NaN value recorded\n",
    "    #after this value then the previous non-NaN value will be taken.\n",
    "    if pd.isna(top): \n",
    "        return bottom\n",
    "    \n",
    "\n",
    "    else:\n",
    "        mean = (top + bottom)/2\n",
    "        return mean\n",
    "\n",
    "#Find previous non-NaN value.\n",
    "def find_bottom(templist, index):\n",
    "    while pd.isna(templist[index-1]):\n",
    "        index = index-1\n",
    "    bottom = templist[index-1]\n",
    "    return bottom\n",
    "\n",
    "#Find next non-NaN value.\n",
    "#If there are no more non-NaN values return the previous non-NaN value.\n",
    "def find_top(templist, index):\n",
    "    while pd.isna(templist[index+1]):\n",
    "        index = index+1\n",
    "        if index == max(templist.index):\n",
    "            top = np.nan\n",
    "            return top\n",
    "    top = templist[index+1]\n",
    "    return top   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code for first run data cleaning, no longer needed after 'rainfall.csv' was created at the end of cleaning process.\n",
    "\n",
    "#rainfall = rainfall_original.copy()\n",
    "\n",
    "##'RISK_MM' was used by creator of dataset to extrapolate response variable, 'RainTomorrow.'  Needs to be dropped to not \n",
    "##influence prediction.\n",
    "#rainfall.drop([\"RISK_MM\"], axis=1, inplace=True)\n",
    "\n",
    "##Drop any observation with no record of rainfall for the day.  Cannot be imputed.\n",
    "#rainfall.dropna(subset=[\"RainToday\"], inplace=True)\n",
    "\n",
    "#Reset the Index of each observation to match it's iloc, get rid of gaps between Index integers.\n",
    "#rainfall = rainfall.reset_index(drop=True)\n",
    "#rainfall.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##can be skipped if rainfall.csv already generated!\n",
    "\n",
    "##set the cardinal directions to degrees.\n",
    "#directions = {'N':0, 'NNE':22.5, 'NE':45, 'NE':45, 'ENE':67.5, 'E':90, 'ESE':112.5, 'SE':135, 'SSE':157.5, 'S':180,\\\n",
    "#              'SSW':202.5, 'SW':225, 'WSW':247.5, 'W':270, 'WNW':292.5, 'NW':315, 'NNW':337.5}\n",
    "\n",
    "##Replace cardianl direction to their corresponding degrees.\n",
    "#rainfall = rainfall.replace(directions) \n",
    "\n",
    "#Get name of all cities in the data frame.\n",
    "#cities = rainfall.Location.unique() \n",
    "\n",
    "#c_variables = []\n",
    "#d_variables = []\n",
    "\n",
    "##change 'Yes' and 'No' to 1 and 0 respectively.\n",
    "#rainfall.RainToday = rainfall.RainToday=='Yes'\n",
    "#rainfall.RainToday = rainfall.RainToday.astype(np.int)\n",
    "#rainfall.RainTomorrow = rainfall.RainTomorrow=='Yes'\n",
    "#rainfall.RainTomorrow = rainfall.RainTomorrow.astype(np.int)\n",
    "\n",
    "##Find all variables with continous data type.\n",
    "#for l in list(rainfall):\n",
    "#    if (rainfall[l].dtypes == 'float64'):\n",
    "#        c_variables.append(l)\n",
    "#    else:\n",
    "#        d_variables.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##can be skipped if rainfall.csv already generated! Very expensive, 'rainfall.csv' can be uploaded from working directory\n",
    "\n",
    "##Impute values to NaN's and save to csv file for later use.\n",
    "#impute_by_city(cities, c_variables)\n",
    "#rainfall.to_csv(\"rainfall.csv\", sep=',', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BadgerysCreek\n",
      "Newcastle\n",
      "NorahHead\n",
      "Penrith\n",
      "Tuggeranong\n",
      "MountGinini\n",
      "Nhil\n",
      "Dartmoor\n",
      "GoldCoast\n",
      "Adelaide\n",
      "Albany\n",
      "Witchcliffe\n",
      "SalmonGums\n",
      "Walpole\n"
     ]
    }
   ],
   "source": [
    "##load pre-generated rainfall.csv file.\n",
    "rainfall = pd.read_csv('rainfall.csv', index_col=0) \n",
    "\n",
    "#Variables 'Evaporation' and 'Sunshine' contained many missing values, too many to be imputed.\n",
    "rainfall = rainfall.drop(['Evaporation', 'Sunshine'], axis = 1)\n",
    "\n",
    "#Get name of all cities in the data frame.\n",
    "l = list(rainfall.Location.unique())\n",
    "\n",
    "#Drop all observations with NaN's.  These are values that could not be imputed using the above code.\n",
    "rainfall.dropna(subset = list(rainfall), inplace = True)\n",
    "\n",
    "#List all cities that were dropped\n",
    "for i in l:\n",
    "    if i not in rainfall.Location.unique():\n",
    "        print(i)\n",
    "        \n",
    "#'Date' and 'Location' variables not needed for prediction. \n",
    "rainfall = rainfall.drop(['Date', 'Location'], axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation 2\n",
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 1\n",
    "Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 2\n",
    "Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit as ss\n",
    "   \n",
    "#Split our data into training and testing sets, 80% of data will be in the training set and 20% the testing set.\n",
    "#Data will be process this way 5 times, value can be change per user's judgement.  It is recommended that number\n",
    "#of iterations be at least 2 so that standard deviations can be computed.\n",
    "num_cv_iterations = 1\n",
    "num_instances = len(y)\n",
    "cv_object = ss(n_splits=num_cv_iterations, test_size  = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 3\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "\n",
    "* Task 1 Classification: Predict if it will rain the next day or not.\n",
    "* Task 2 Regression: Compute how much it rained that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Classification:  Predit if it will rain the next day or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN with 1 neighbors with 'uniform' weights is: 0.80731\n",
      "Time to Run: 18.088132619857788\n",
      "Accuracy of KNN with 2 neighbors with 'uniform' weights is: 0.82991\n",
      "Time to Run: 21.955844402313232\n",
      "Accuracy of KNN with 3 neighbors with 'uniform' weights is: 0.83302\n",
      "Time to Run: 24.111968755722046\n",
      "Accuracy of KNN with 4 neighbors with 'uniform' weights is: 0.83973\n",
      "Time to Run: 27.3185977935791\n",
      "Accuracy of KNN with 5 neighbors with 'uniform' weights is: 0.84284\n",
      "Time to Run: 27.238258838653564\n",
      "Accuracy of KNN with 6 neighbors with 'uniform' weights is: 0.84531\n",
      "Time to Run: 28.14395046234131\n",
      "Accuracy of KNN with 7 neighbors with 'uniform' weights is: 0.84792\n",
      "Time to Run: 28.96383023262024\n",
      "Accuracy of KNN with 8 neighbors with 'uniform' weights is: 0.84916\n",
      "Time to Run: 29.89581799507141\n",
      "Accuracy of KNN with 9 neighbors with 'uniform' weights is: 0.85049\n",
      "Time to Run: 31.055959701538086\n",
      "Accuracy of KNN with 10 neighbors with 'uniform' weights is: 0.85093\n",
      "Time to Run: 32.8038604259491\n",
      "Accuracy of KNN with 1 neighbors with 'distance' weights is: 0.80731\n",
      "Time to Run: 18.46953797340393\n",
      "Accuracy of KNN with 2 neighbors with 'distance' weights is: 0.80731\n",
      "Time to Run: 22.25392985343933\n",
      "Accuracy of KNN with 3 neighbors with 'distance' weights is: 0.83337\n",
      "Time to Run: 24.5742506980896\n",
      "Accuracy of KNN with 4 neighbors with 'distance' weights is: 0.83559\n",
      "Time to Run: 27.7421932220459\n",
      "Accuracy of KNN with 5 neighbors with 'distance' weights is: 0.84363\n",
      "Time to Run: 27.166878700256348\n",
      "Accuracy of KNN with 6 neighbors with 'distance' weights is: 0.84516\n",
      "Time to Run: 28.24410080909729\n",
      "Accuracy of KNN with 7 neighbors with 'distance' weights is: 0.84856\n",
      "Time to Run: 29.154850721359253\n",
      "Accuracy of KNN with 8 neighbors with 'distance' weights is: 0.84995\n",
      "Time to Run: 33.97143864631653\n",
      "Accuracy of KNN with 9 neighbors with 'distance' weights is: 0.85083\n",
      "Time to Run: 30.63287353515625\n",
      "Accuracy of KNN with 10 neighbors with 'distance' weights is: 0.85192\n",
      "Time to Run: 31.32528853416443\n",
      "Accuracy of Gausian Naive Bayes is: 0.81861\n",
      "Time to Run: 0.08035039901733398\n",
      "Accuracy of Logistic Regression 'l1' penalty with cost 1 is: 0.85458\n",
      "Time to Run: 1.436908483505249\n",
      "Accuracy of Logistic Regression 'l1' penalty with cost 2 is: 0.85468\n",
      "Time to Run: 1.3729240894317627\n",
      "Accuracy of Logistic Regression 'l1' penalty with cost 3 is: 0.85468\n",
      "Time to Run: 1.4572436809539795\n",
      "Accuracy of Logistic Regression 'l1' penalty with cost 4 is: 0.85468\n",
      "Time to Run: 1.1675806045532227\n",
      "Accuracy of Logistic Regression 'l1' penalty with cost 5 is: 0.85468\n",
      "Time to Run: 1.4264917373657227\n",
      "Accuracy of Logistic Regression 'l1' penalty with cost 6 is: 0.85468\n",
      "Time to Run: 1.4145903587341309\n",
      "Accuracy of Logistic Regression 'l1' penalty with cost 7 is: 0.85468\n",
      "Time to Run: 1.2221400737762451\n",
      "Accuracy of Logistic Regression 'l1' penalty with cost 8 is: 0.85468\n",
      "Time to Run: 1.5336294174194336\n",
      "Accuracy of Logistic Regression 'l1' penalty with cost 9 is: 0.85468\n",
      "Time to Run: 1.4324424266815186\n",
      "Accuracy of Logistic Regression 'l1' penalty with cost 10 is: 0.85468\n",
      "Time to Run: 1.3243160247802734\n",
      "Accuracy of Logistic Regression 'l2' penalty with cost 1 is: 0.85468\n",
      "Time to Run: 0.6839821338653564\n",
      "Accuracy of Logistic Regression 'l2' penalty with cost 2 is: 0.85478\n",
      "Time to Run: 0.6770379543304443\n",
      "Accuracy of Logistic Regression 'l2' penalty with cost 3 is: 0.85478\n",
      "Time to Run: 0.6795179843902588\n",
      "Accuracy of Logistic Regression 'l2' penalty with cost 4 is: 0.85473\n",
      "Time to Run: 0.6666224002838135\n",
      "Accuracy of Logistic Regression 'l2' penalty with cost 5 is: 0.85473\n",
      "Time to Run: 0.6904301643371582\n",
      "Accuracy of Logistic Regression 'l2' penalty with cost 6 is: 0.85473\n",
      "Time to Run: 0.6700944900512695\n",
      "Accuracy of Logistic Regression 'l2' penalty with cost 7 is: 0.85473\n",
      "Time to Run: 0.6998536586761475\n",
      "Accuracy of Logistic Regression 'l2' penalty with cost 8 is: 0.85473\n",
      "Time to Run: 0.6909255981445312\n",
      "Accuracy of Logistic Regression 'l2' penalty with cost 9 is: 0.85473\n",
      "Time to Run: 0.69687819480896\n",
      "Accuracy of Logistic Regression 'l2' penalty with cost 10 is: 0.85473\n",
      "Time to Run: 0.6844782829284668\n",
      "\n",
      "The maximum accuracy of 0.85473 is achieved with Logistic Regression 'l2' penalty with cost 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import StandardScaler as sts\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.neighbors import KNeighborsClassifier as knnc\n",
    "from sklearn.naive_bayes import GaussianNB as gnb\n",
    "import time\n",
    "\n",
    "#Assign values to response variable, y, and explanatory variables, x.\n",
    "temp_rainfall = rainfall.copy()\n",
    "if 'RainTomorrow' in temp_rainfall:\n",
    "    #Response variable is 'RainTomorrow'\n",
    "    y = temp_rainfall['RainTomorrow'].values\n",
    "    \n",
    "    #Remove response variable from dataframe\n",
    "    del temp_rainfall['RainTomorrow']\n",
    "    \n",
    "    #Everything else is the explanatory variables used in prediction.\n",
    "    x = temp_rainfall.values\n",
    "    \n",
    "scl_obj = sts()\n",
    "accuracy_dict = {}\n",
    "\n",
    "#Split the data into training and testing set 5 different ways, iterate through each way.\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(x,y)):\n",
    "    \n",
    "    #Standardize the explanatory variables of the training and testing sets' means to be around 0 with a standard deviation of 1.  \n",
    "    #Each value is subtracted from the mean and divided by the standard deviation of the whole dataset.\n",
    "    scl_obj.fit(x[train_indices])\n",
    "    X_train_scaled = scl_obj.transform(x[train_indices])\n",
    "    X_test_scaled = scl_obj.transform(x[test_indices])\n",
    "\n",
    "    #K Nearest Neighbors, all neighbors are treated the same.  Number of neighbors range from 1 to 10.\n",
    "    for K in range(1, 11):\n",
    "        t0=time.time()\n",
    "        knn_clf = knn(n_neighbors=K, weights='uniform')\n",
    "        knn_clf.fit(X_train_scaled,y[train_indices])\n",
    "        y_hat = knn_clf.predict(X_test_scaled)\n",
    "        acc = mt.accuracy_score(y[test_indices],y_hat)\n",
    "        accuracy_dict.update({'KNN with %d neighbors with \\'uniform\\' weights' %(K): acc})\n",
    "        print('Accuracy of KNN with %d neighbors with \\'uniform\\' weights is: %.5f'%(K,acc))\n",
    "        print (\"Time to Run:\", time.time()-t0)\n",
    "        \n",
    "    #K Nearest Neighbors, closer neighbors are given more weight.  Number of neighbors range from 1 to 10.\n",
    "    for K in range(1, 11):\n",
    "        t0=time.time()\n",
    "        knn_clf = knnc(n_neighbors=K, weights='distance')\n",
    "        knn_clf.fit(X_train_scaled,y[train_indices])\n",
    "        y_hat = knn_clf.predict(X_test_scaled)\n",
    "        acc = mt.accuracy_score(y[test_indices],y_hat)\n",
    "        accuracy_dict.update({'KNN with %d neighbors with \\'distance\\' weights' %(K): acc})\n",
    "        print('Accuracy of KNN with %d neighbors with \\'distance\\' weights is: %.5f'%(K,acc))\n",
    "        print (\"Time to Run:\", time.time()-t0)\n",
    "           \n",
    "    #Gaussian Naive Bayes\n",
    "    t0=time.time()\n",
    "    gnb_clf = gnb()\n",
    "    gnb_clf.fit(X_train_scaled,y[train_indices])\n",
    "    y_hat = gnb_clf.predict(X_test_scaled)\n",
    "    acc = mt.accuracy_score(y[test_indices],y_hat)\n",
    "    accuracy_dict.update({'Gausian Naive Bayes': acc})\n",
    "    print('Accuracy of Gausian Naive Bayes is: %.5f' %(acc))\n",
    "    print (\"Time to Run:\", time.time()-t0)\n",
    "    \n",
    "    \n",
    "    #Perform Logistic Regression, cost (inverse of regulation strength) range from 1 to 10 and l1 penalty.\n",
    "    for cost in range(1,11):\n",
    "        t0=time.time()\n",
    "        lr_clf = lr(penalty='l1', C=cost)\n",
    "        lr_clf.fit(X_train_scaled,y[train_indices])\n",
    "        y_hat = lr_clf.predict(X_test_scaled)\n",
    "        acc = mt.accuracy_score(y[test_indices],y_hat)\n",
    "        accuracy_dict.update({'Logistic Regression \\'l1\\' penalty with cost %d' %(cost): acc})\n",
    "        print('Accuracy of Logistic Regression \\'l1\\' penalty with cost %d is: %.5f' %(cost, acc))\n",
    "        print (\"Time to Run:\", time.time()-t0)\n",
    "    \n",
    "    #Perform Logistic Regression, cost (inverse of regulation strength) range from 1 to 10 and l2 penalty.\n",
    "    for cost in range(1,11):\n",
    "        t0=time.time()\n",
    "        lr_clf = lr(penalty='l2', C=cost)\n",
    "        lr_clf.fit(X_train_scaled,y[train_indices])\n",
    "        y_hat = lr_clf.predict(X_test_scaled)\n",
    "        acc = mt.accuracy_score(y[test_indices],y_hat)\n",
    "        accuracy_dict.update({'Logistic Regression \\'l2\\' penalty with cost %d' %(cost): acc})\n",
    "        print('Accuracy of Logistic Regression \\'l2\\' penalty with cost %d is: %.5f' %(cost, acc))\n",
    "        print (\"Time to Run:\", time.time()-t0)\n",
    "\n",
    "print()\n",
    "print('The maximum accuracy of %.5f is achieved with %s' %(max(accuracy_dict.items())[1], max(accuracy_dict.items())[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Regression: Compute how much it rained that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor as knnr\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.tree import DecisionTreeRegressor as dtr\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "if 'Rainfall' in temp_rainfall:\n",
    "    #Response variable is 'RainTomorrow'\n",
    "    y = temp_rainfall['Rainfall'].values\n",
    "    \n",
    "    #Remove response variable from dataframe\n",
    "    del temp_rainfall['Rainfall']\n",
    "    \n",
    "    #Everything else is the explanatory variables used in prediction.\n",
    "    x = temp_rainfall.values\n",
    "    \n",
    "scl_obj = sts()\n",
    "\n",
    "#Split the data into training and testing set 5 different ways, iterate through each way.\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(x,y)):\n",
    "    \n",
    "    #Standardize the explanatory variables of the training and testing sets' means to be around 0 with a standard deviation of 1.  \n",
    "    #Each value is subtracted from the mean and divided by the standard deviation of the whole dataset.\n",
    "    scl_obj.fit(x[train_indices])\n",
    "    X_train_scaled = scl_obj.transform(x[train_indices])\n",
    "    X_test_scaled = scl_obj.transform(x[test_indices])\n",
    "    \n",
    "    rmse_val = []\n",
    "    for K in range(1,31):\n",
    "        t0=time.time()\n",
    "        knnr_clf = knnr(n_neighbors=K, weights='uniform')\n",
    "        knnr_clf.fit(X_train_scaled,y[train_indices])\n",
    "        y_hat = knnr_clf.predict(X_test_scaled)\n",
    "        error = np.sqrt(mse(y[test_indices],y_hat))\n",
    "        rmse_val.append(error)\n",
    "        print('RMSE of KNN with %d neighbors with \\'uniform\\' weights is: %.5f'%(K,error))\n",
    "        print (\"Time to Run:\", time.time()-t0)\n",
    "    #plotting the rmse values against k values\n",
    "    curve = pd.DataFrame(rmse_val) #elbow curve \n",
    "    curve.plot()\n",
    "    print('The number of neighbors with the lowest RMSE is %d, RMSE of %.5f.' %(rmse_val.index(min(rmse_val))+1,min(rmse_val)))\n",
    "    \n",
    "    rmse_val = []\n",
    "    for K in range(1,31):\n",
    "        t0=time.time()\n",
    "        knnr_clf = knnr(n_neighbors=K, weights='distance')\n",
    "        knnr_clf.fit(X_train_scaled,y[train_indices])\n",
    "        y_hat = knnr_clf.predict(X_test_scaled)\n",
    "        error = np.sqrt(mse(y[test_indices],y_hat))\n",
    "        rmse_val.append(error)\n",
    "        print('RMSE of KNN with %d neighbors with \\'distance\\' weights is: %.5f'%(K,error))\n",
    "        print (\"Time to Run:\", time.time()-t0)\n",
    "    #plotting the rmse values against k values\n",
    "    curve = pd.DataFrame(rmse_val) #elbow curve \n",
    "    curve.plot()\n",
    "    print('The number of neighbors with the lowest RMSE is %d, RMSE of %.5f.' %(rmse_val.index(min(rmse_val))+1,min(rmse_val)))\n",
    "    \n",
    "    rmse_val = []\n",
    "    for K in range(2,10):\n",
    "        t0=time.time()\n",
    "        dtr_clf = dtr(max_depth = K)\n",
    "        dtr_clf.fit(X_train_scaled,y[train_indices])\n",
    "        y_hat = knnr_clf.predict(X_test_scaled)        \n",
    "        error = np.sqrt(mse(y[test_indices],y_hat))\n",
    "        rmse_val.append(error)\n",
    "        print('RMSE of Decision Tree with %d depth is: %.5f'%(K,error))\n",
    "        print (\"Time to Run:\", time.time()-t0)\n",
    "    #plotting the rmse values against k values\n",
    "    curve = pd.DataFrame(rmse_val) #elbow curve \n",
    "    curve.plot()\n",
    "    print('The depth with the lowest RMSE is %d, RMSE of %.5f.' %(rmse_val.index(min(rmse_val))+1,min(rmse_val)))\n",
    "\n",
    "    rmse_val = []\n",
    "    for e in np.arange(.1,1,.1):\n",
    "        t0=time.time()\n",
    "        svr_clf = SVR(epsilon=e)\n",
    "        svr_clf.fit(X_train_scaled,y[train_indices])\n",
    "        y_hat = svr_clf.predict(X_test_scaled)\n",
    "        error = np.sqrt(mse(y[test_indices],y_hat))\n",
    "        rmse_val.append(error)\n",
    "        print('RMSE of Support Vector Regression with epsilon = %.1f is: %.5f'%(e,error))\n",
    "        print (\"Time to Run:\", time.time()-t0)\n",
    "    #plotting the rmse values against k values\n",
    "    curve = pd.DataFrame(rmse_val) #elbow curve \n",
    "    curve.plot()\n",
    "    print('The depth with the lowest RMSE is %d, RMSE of %.5f.' %(rmse_val.index(min(rmse_val))+1,min(rmse_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 4\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 5\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniquesâ€”be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 6\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
